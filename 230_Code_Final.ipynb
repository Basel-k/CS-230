{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvdAxwRQj4-K"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip -q install -U tensorflow-datasets scikit-learn opencv-python-headless tqdm\n",
        "\n",
        "# Imports\n",
        "import random, numpy as np, matplotlib.pyplot as plt, torch, torch.nn as nn, cv2, tensorflow as tf, tensorflow_datasets as tfds\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from torchvision.transforms import ColorJitter\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tqdm.auto import tqdm\n",
        "try: tf.config.set_visible_devices([], \"GPU\")\n",
        "except Exception: pass\n",
        "CFG = dict(seed=42, epochs=2, steps_per_epoch=350, lr=1e-4, weight_decay=1e-4,\n",
        "           val_batches=120, test_batches=120, tsi_N=2000, tsi_m=3,\n",
        "           tile_sizes=(8,12,16), jitter=dict(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.02),\n",
        "           viz_k=5)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(\"Device:\", device)\n",
        "bs = 256 if device.type == \"cuda\" else 64; print(\"Batch size:\", bs)\n",
        "# Seeds\n",
        "def seed_everything(s):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "seed_everything(CFG[\"seed\"])\n",
        "# ----------------------------\n",
        "# Load PCam from TFDS\n",
        "# ----------------------------\n",
        "print(\"Loading PCam from TFDS...\")\n",
        "train_tf = tfds.load(\"patch_camelyon\", split=\"train\", as_supervised=True, shuffle_files=True)\n",
        "val_tf   = tfds.load(\"patch_camelyon\", split=\"validation\", as_supervised=True, shuffle_files=False)\n",
        "test_tf  = tfds.load(\"patch_camelyon\", split=\"test\", as_supervised=True, shuffle_files=False)\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_tf = train_tf.shuffle(4096, seed=CFG[\"seed\"], reshuffle_each_iteration=True).batch(bs).prefetch(AUTOTUNE)\n",
        "val_tf   = val_tf.batch(bs).prefetch(AUTOTUNE)\n",
        "test_tf  = test_tf.batch(bs).prefetch(AUTOTUNE)\n",
        "# ----------------------------\n",
        "# Preprocess (ResNet with ImageNet normalization)\n",
        "# ----------------------------\n",
        "IMNET_MEAN = torch.tensor([0.485,0.456,0.406], device=device).view(1,3,1,1)\n",
        "IMNET_STD  = torch.tensor([0.229,0.224,0.225], device=device).view(1,3,1,1)\n",
        "def preprocess_uint8_batch(b):\n",
        "    x = torch.from_numpy(b).to(device=device, dtype=torch.float32)/255.0\n",
        "    x = x.permute(0,3,1,2).contiguous()\n",
        "    return (x-IMNET_MEAN)/IMNET_STD\n",
        "@torch.no_grad()\n",
        "def logits_from_uint8_batch(b, sub_bs=1024):\n",
        "    \"\"\"Same helper: uint8 NHWC -> logits (N,)\"\"\"\n",
        "    model.eval(); out=[]\n",
        "    for i in range(0, b.shape[0], sub_bs):\n",
        "        out.append(model(preprocess_uint8_batch(b[i:i+sub_bs])).detach().float().cpu().numpy().reshape(-1))\n",
        "    return np.concatenate(out, axis=0)\n",
        "def prob_from_logit(z): return 1.0/(1.0+np.exp(-z))\n",
        "# ----------------------------\n",
        "# Model: ResNet18 pretrained -> binary head\n",
        "# ----------------------------\n",
        "print(\"Building model...\")\n",
        "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "model.fc = nn.Linear(model.fc.in_features, 1)\n",
        "model = model.to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=CFG[\"lr\"], weight_decay=CFG[\"weight_decay\"])\n",
        "# ----------------------------\n",
        "#  eval helper\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate(tf_dataset, max_batches):\n",
        "    model.eval(); logits_all, y_all = [], []\n",
        "    for b, (img, y) in enumerate(tfds.as_numpy(tf_dataset)):\n",
        "        if b >= max_batches: break\n",
        "        x = preprocess_uint8_batch(img)\n",
        "        y_t = torch.from_numpy(y).to(device=device, dtype=torch.float32).view(-1, 1)  # unused, but kept as-is\n",
        "        logits = model(x).detach().cpu().numpy().reshape(-1)\n",
        "        logits_all.append(logits); y_all.append(y.astype(np.int32).reshape(-1))\n",
        "    logits = np.concatenate(logits_all) if logits_all else np.array([])\n",
        "    y_true = np.concatenate(y_all) if y_all else np.array([])\n",
        "    if len(y_true) == 0: return dict(acc=np.nan, auroc=np.nan, n=0)\n",
        "    p = prob_from_logit(logits); pred = (p >= 0.5).astype(np.int32)\n",
        "    acc = (pred == y_true).mean()\n",
        "    try: auroc = roc_auc_score(y_true, p)\n",
        "    except Exception: auroc = np.nan\n",
        "    return dict(acc=float(acc), auroc=float(auroc), n=int(len(y_true)))\n",
        "# ----------------------------\n",
        "# Train\n",
        "# ----------------------------\n",
        "history = {\"train_loss\": [], \"val_acc\": [], \"val_auroc\": []}\n",
        "print(\"\\nTraining...\")\n",
        "train_iter = iter(tfds.as_numpy(train_tf.repeat()))\n",
        "for ep in range(1, CFG[\"epochs\"]+1):\n",
        "    model.train(); losses=[]\n",
        "    for _ in tqdm(range(CFG[\"steps_per_epoch\"]), desc=f\"epoch {ep}/{CFG['epochs']}\", leave=False):\n",
        "        img, y = next(train_iter)\n",
        "        x = preprocess_uint8_batch(img)\n",
        "        y_t = torch.from_numpy(y).to(device=device, dtype=torch.float32).view(-1, 1)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y_t)\n",
        "        loss.backward(); opt.step()\n",
        "        losses.append(loss.item())\n",
        "    val = evaluate(val_tf, CFG[\"val_batches\"])\n",
        "    history[\"train_loss\"].append(float(np.mean(losses)))\n",
        "    history[\"val_acc\"].append(val[\"acc\"])\n",
        "    history[\"val_auroc\"].append(val[\"auroc\"])\n",
        "    print(f\"Epoch {ep:02d} | train_loss={history['train_loss'][-1]:.4f} | val_acc={val['acc']:.4f} | val_AUROC={val['auroc']:.4f} (n={val['n']})\")\n",
        "test = evaluate(test_tf, CFG[\"test_batches\"])\n",
        "print(f\"\\nTest (limited to {CFG['test_batches']} batches) | acc={test['acc']:.4f} | AUROC={test['auroc']:.4f} (n={test['n']})\")\n",
        "# ----------------------------\n",
        "# Plots: training curves\n",
        "# ----------------------------\n",
        "plt.figure(); plt.plot(range(1, CFG[\"epochs\"]+1), history[\"train_loss\"], marker=\"o\")\n",
        "plt.title(\"Training loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Train loss (BCEWithLogits)\"); plt.grid(True); plt.show()\n",
        "plt.figure(); plt.plot(range(1, CFG[\"epochs\"]+1), history[\"val_acc\"], marker=\"o\", label=\"Val Acc\")\n",
        "plt.plot(range(1, CFG[\"epochs\"]+1), history[\"val_auroc\"], marker=\"o\", label=\"Val AUROC\")\n",
        "plt.title(\"Validation metrics\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Metric\"); plt.grid(True); plt.legend(); plt.show()\n",
        "# ============================================================\n",
        "# TSI bits: S(x), T(x), score, and TSI = score(S)-score(T)\n",
        "# ============================================================\n",
        "def shape_view_uint8(img, rng):\n",
        "    # \"Shape-ish\" view: smooth texture but keep edges (kinda like sketching the patch)\n",
        "    d = int(rng.integers(5, 10))\n",
        "    sm = cv2.bilateralFilter(img, d=d, sigmaColor=float(rng.uniform(40.0, 90.0)), sigmaSpace=float(rng.uniform(40.0, 90.0)))\n",
        "    gray = cv2.cvtColor(sm, cv2.COLOR_RGB2GRAY)\n",
        "    mag = cv2.magnitude(cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=3), cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=3))\n",
        "    mag = mag/(mag.max()+1e-6)\n",
        "    alpha = float(rng.uniform(0.15, 0.35))\n",
        "    out = sm.astype(np.float32) + alpha * (mag[..., None] * 255.0).astype(np.float32)\n",
        "    return np.clip(out, 0, 255).astype(np.uint8)\n",
        "def texture_view_uint8(img, rng, tile_sizes=(8,12,16)):\n",
        "    # \"Texture-ish\" view: shuffle tiles so local stats stay but global layout gets wrecked\n",
        "    H, W, C = img.shape\n",
        "    s = int(rng.choice(np.array(tile_sizes)))\n",
        "    h, w = (H//s)*s, (W//s)*s\n",
        "    core = img[:h, :w]\n",
        "    nH, nW = h//s, w//s\n",
        "    tiles = core.reshape(nH, s, nW, s, C).transpose(0,2,1,3,4).reshape(nH*nW, s, s, C)\n",
        "    tiles = tiles[rng.permutation(nH*nW)]\n",
        "    shuffled = tiles.reshape(nH, nW, s, s, C).transpose(0,2,1,3,4).reshape(h, w, C)\n",
        "    out = img.copy(); out[:h, :w] = shuffled\n",
        "    return out\n",
        "def true_class_score(logit, y01): return np.where(y01 == 1, logit, -logit)\n",
        "# ----------------------------\n",
        "# Pull a validation subset for per-image analysis\n",
        "# ----------------------------\n",
        "print(\"\\nSampling validation subset for TSI...\")\n",
        "raw_imgs, raw_y = [], []\n",
        "for img, y in tfds.as_numpy(val_tf.unbatch().take(CFG[\"tsi_N\"])): raw_imgs.append(img); raw_y.append(int(y))\n",
        "raw_imgs = np.stack(raw_imgs, axis=0); raw_y = np.array(raw_y, dtype=np.int32)\n",
        "# Base logits/preds on the subset (same math)\n",
        "base_logit = logits_from_uint8_batch(raw_imgs)\n",
        "base_prob  = prob_from_logit(base_logit)\n",
        "base_pred  = (base_prob >= 0.5).astype(np.int32)\n",
        "base_corr  = (base_pred == raw_y).astype(np.int32)\n",
        "err        = 1 - base_corr\n",
        "conf       = np.maximum(base_prob, 1.0 - base_prob)\n",
        "print(f\"Subset base accuracy: {base_corr.mean():.4f} (N={len(raw_y)})\")\n",
        "# ----------------------------\n",
        "# Tiny sanity checks\n",
        "# ----------------------------\n",
        "rng_test = np.random.default_rng(CFG[\"seed\"])\n",
        "img0 = raw_imgs[0]\n",
        "s0 = shape_view_uint8(img0, rng_test)\n",
        "t0 = texture_view_uint8(img0, rng_test, CFG[\"tile_sizes\"])\n",
        "assert s0.shape == img0.shape == t0.shape, \"Transform shape mismatch\"\n",
        "assert s0.dtype == np.uint8 and t0.dtype == np.uint8, \"Transforms must return uint8\"\n",
        "assert 0 <= s0.min() and s0.max() <= 255 and 0 <= t0.min() and t0.max() <= 255, \"Transforms out of [0,255]\"\n",
        "def hf_energy(img):\n",
        "    g = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    lap = cv2.Laplacian(g, cv2.CV_32F)\n",
        "    return float(np.mean(lap*lap))\n",
        "E_x, E_s, E_t = [], [], []\n",
        "for i in range(32):\n",
        "    rng = np.random.default_rng(int(rng_test.integers(0, 2**32-1)))\n",
        "    xi = raw_imgs[i]\n",
        "    E_x.append(hf_energy(xi)); E_s.append(hf_energy(shape_view_uint8(xi, rng))); E_t.append(hf_energy(texture_view_uint8(xi, rng, CFG[\"tile_sizes\"])))\n",
        "print(f\"[Diagnostic] mean HF energy: orig={np.mean(E_x):.2f}, S(x)={np.mean(E_s):.2f}, T(x)={np.mean(E_t):.2f}\")\n",
        "print(f\"[Diagnostic] per-channel mean diff (orig vs T): {np.mean(np.abs(raw_imgs[:32].mean((1,2)) - np.stack([texture_view_uint8(raw_imgs[i], np.random.default_rng(i), CFG['tile_sizes']).mean((0,1)) for i in range(32)])), axis=0)}\")\n",
        "# ----------------------------\n",
        "# Compute TSI\n",
        "# ----------------------------\n",
        "print(\"\\nComputing TSI...\")\n",
        "N = len(raw_imgs)\n",
        "tsi = np.zeros(N, dtype=np.float32)\n",
        "scoreS = np.zeros(N, dtype=np.float32)\n",
        "scoreT = np.zeros(N, dtype=np.float32)\n",
        "rng_master = np.random.default_rng(CFG[\"seed\"])\n",
        "chunk = 64\n",
        "for i0 in tqdm(range(0, N, chunk), desc=\"TSI\", leave=False):\n",
        "    i1 = min(N, i0 + chunk)\n",
        "    imgs, ys = raw_imgs[i0:i1], raw_y[i0:i1]\n",
        "    S_views, T_views = [], []\n",
        "    for j in range(i1 - i0):\n",
        "        for _ in range(CFG[\"tsi_m\"]):\n",
        "            rng = np.random.default_rng(int(rng_master.integers(0, 2**32-1)))\n",
        "            S_views.append(shape_view_uint8(imgs[j], rng))\n",
        "            T_views.append(texture_view_uint8(imgs[j], rng, CFG[\"tile_sizes\"]))\n",
        "    S_views, T_views = np.stack(S_views, axis=0), np.stack(T_views, axis=0)\n",
        "    logS, logT = logits_from_uint8_batch(S_views), logits_from_uint8_batch(T_views)\n",
        "    ys_rep = np.repeat(ys, CFG[\"tsi_m\"])\n",
        "    scS = true_class_score(logS, ys_rep).reshape(-1, CFG[\"tsi_m\"]).mean(axis=1)\n",
        "    scT = true_class_score(logT, ys_rep).reshape(-1, CFG[\"tsi_m\"]).mean(axis=1)\n",
        "    scoreS[i0:i1], scoreT[i0:i1], tsi[i0:i1] = scS.astype(np.float32), scT.astype(np.float32), (scS-scT).astype(np.float32)\n",
        "print(\"\\nTSI summary:\"); print(f\"  N={N}\")\n",
        "print(f\"  mean={tsi.mean():.4f} | std={tsi.std():.4f} | min={tsi.min():.4f} | max={tsi.max():.4f}\")\n",
        "plt.figure(); plt.hist(tsi, bins=50)\n",
        "plt.title(\"TSI distribution (validation subset)\")\n",
        "plt.xlabel(\"TSI = score(S(x)) - score(T(x))\"); plt.ylabel(\"Count\")\n",
        "plt.grid(True); plt.show()\n",
        "# ----------------------------\n",
        "# Deciles (TSI and |TSI|)\n",
        "# ----------------------------\n",
        "def decile_bins(values, n_bins=10):\n",
        "    idx = np.argsort(values); bins = np.zeros(len(values), dtype=np.int32)\n",
        "    for b in range(n_bins):\n",
        "        lo = int(round(b * len(values) / n_bins)); hi = int(round((b + 1) * len(values) / n_bins))\n",
        "        bins[idx[lo:hi]] = b\n",
        "    return bins\n",
        "bins_tsi = decile_bins(tsi, 10)\n",
        "bins_abs = decile_bins(np.abs(tsi), 10)\n",
        "acc_by = [float(base_corr[bins_tsi == b].mean()) for b in range(10)]\n",
        "print(\"\\nAccuracy by TSI decile (0=lowest → 9=highest):\")\n",
        "for b, a in enumerate(acc_by): print(f\"  decile {b}: acc={a:.4f}\")\n",
        "plt.figure(); plt.plot(range(10), acc_by, marker=\"o\")\n",
        "plt.title(\"Base accuracy by TSI decile (can be non-monotone)\")\n",
        "plt.xlabel(\"TSI decile (0 low → 9 high)\"); plt.ylabel(\"Accuracy\")\n",
        "plt.ylim(0, 1.0); plt.grid(True); plt.show()\n",
        "acc_abs = [float(base_corr[bins_abs == b].mean()) for b in range(10)]\n",
        "plt.figure(); plt.plot(range(10), acc_abs, marker=\"o\")\n",
        "plt.title(\"Base accuracy by |TSI| decile (diagnostic for U-shape)\")\n",
        "plt.xlabel(\"|TSI| decile (0 small → 9 large)\"); plt.ylabel(\"Accuracy\")\n",
        "plt.ylim(0, 1.0); plt.grid(True); plt.show()\n",
        "print(f\"\\nPearson corr(TSI, correctness): {np.corrcoef(tsi, base_corr.astype(np.float32))[0,1]:.4f}\")\n",
        "# ----------------------------\n",
        "# Error detection ROC: TSI vs -|TSI| vs confidence (+ tiny LR baseline)\n",
        "# ----------------------------\n",
        "score_tsi  = tsi.astype(np.float64)\n",
        "score_abs  = (-np.abs(tsi)).astype(np.float64)     # small |TSI| => \"meh/ambiguous\" => riskier\n",
        "score_conf = (-conf).astype(np.float64)            # low confidence => risky\n",
        "au_tsi  = roc_auc_score(err, score_tsi)\n",
        "au_abs  = roc_auc_score(err, score_abs)\n",
        "au_conf = roc_auc_score(err, score_conf)\n",
        "X = np.stack([tsi, conf], axis=1)\n",
        "perm = np.random.default_rng(CFG[\"seed\"]).permutation(N)\n",
        "split = int(0.7 * N); tr, te = perm[:split], perm[split:]\n",
        "lr = LogisticRegression(max_iter=2000); lr.fit(X[tr], err[tr])\n",
        "score_lr = lr.predict_proba(X[te])[:, 1]\n",
        "au_lr = roc_auc_score(err[te], score_lr)\n",
        "print(\"\\nError-detection AUROC (higher is better):\")\n",
        "print(f\"  TSI:        {au_tsi:.4f}\")\n",
        "print(f\"  -|TSI|:     {au_abs:.4f}\")\n",
        "print(f\"  Confidence: {au_conf:.4f}\")\n",
        "print(f\"  LR(TSI,conf) on held-out split: {au_lr:.4f}\")\n",
        "fpr1, tpr1, _ = roc_curve(err, score_tsi)\n",
        "fpr2, tpr2, _ = roc_curve(err, score_abs)\n",
        "fpr3, tpr3, _ = roc_curve(err, score_conf)\n",
        "fpr4, tpr4, _ = roc_curve(err[te], score_lr)\n",
        "plt.figure()\n",
        "plt.plot(fpr1, tpr1, label=f\"TSI (AUROC={au_tsi:.3f})\")\n",
        "plt.plot(fpr2, tpr2, label=f\"-|TSI| (AUROC={au_abs:.3f})\")\n",
        "plt.plot(fpr3, tpr3, label=f\"Confidence (AUROC={au_conf:.3f})\")\n",
        "plt.plot(fpr4, tpr4, label=f\"LR(TSI,conf) held-out (AUROC={au_lr:.3f})\")\n",
        "plt.plot([0,1],[0,1], linestyle=\"--\")\n",
        "plt.title(\"Error detection ROC (validation subset)\")\n",
        "plt.xlabel(\"False positive rate\"); plt.ylabel(\"True positive rate\")\n",
        "plt.grid(True); plt.legend(); plt.show()\n",
        "# ----------------------------\n",
        "# Risk–coverage curves (base errors)\n",
        "# ----------------------------\n",
        "def risk_coverage(error01, keep_order):\n",
        "    n = len(error01)\n",
        "    cum_err = np.cumsum(error01[keep_order])\n",
        "    k = np.arange(1, n+1)\n",
        "    return k/n, cum_err/k\n",
        "order_conf = np.argsort(-conf)            # keep safe first\n",
        "order_abs  = np.argsort(-np.abs(tsi))     # keep extreme |TSI| first\n",
        "cov_c, risk_c = risk_coverage(err, order_conf)\n",
        "cov_a, risk_a = risk_coverage(err, order_abs)\n",
        "plt.figure()\n",
        "plt.plot(cov_c, risk_c, label=\"Keep-high-confidence first\")\n",
        "plt.plot(cov_a, risk_a, label=\"Keep-high-|TSI| first\")\n",
        "plt.title(\"Risk–Coverage (base errors)\")\n",
        "plt.xlabel(\"Coverage (fraction kept)\"); plt.ylabel(\"Risk (error rate on kept)\")\n",
        "plt.grid(True); plt.legend(); plt.show()\n",
        "# ============================================================\n",
        "# Robustness probe: ColorJitter + stratify drop by TSI decile\n",
        "# ============================================================\n",
        "print(\"\\nRobustness probe: ColorJitter on the same subset...\")\n",
        "jitter = ColorJitter(**CFG[\"jitter\"])\n",
        "def apply_jitter_uint8(img):\n",
        "    # torchvision jitter wants torch CHW float in [0,1]\n",
        "    t = torch.from_numpy(img).float()/255.0\n",
        "    t = jitter(t.permute(2,0,1))\n",
        "    t = torch.clamp(t, 0.0, 1.0).permute(1,2,0).numpy()\n",
        "    return (t*255.0 + 0.5).astype(np.uint8)\n",
        "j_imgs = np.stack([apply_jitter_uint8(raw_imgs[i]) for i in tqdm(range(N), desc=\"jitter\", leave=False)], axis=0)\n",
        "j_logit = logits_from_uint8_batch(j_imgs)\n",
        "j_prob  = prob_from_logit(j_logit)\n",
        "j_pred  = (j_prob >= 0.5).astype(np.int32)\n",
        "j_corr  = (j_pred == raw_y).astype(np.int32)\n",
        "acc_orig_by, acc_jit_by, drop_by = [], [], []\n",
        "for b in range(10):\n",
        "    m = (bins_tsi == b)\n",
        "    ao = float(base_corr[m].mean()); aj = float(j_corr[m].mean())\n",
        "    acc_orig_by.append(ao); acc_jit_by.append(aj); drop_by.append(ao-aj)\n",
        "print(\"\\nAccuracy by TSI decile under style-only perturbation:\")\n",
        "for b in range(10):\n",
        "    print(f\"  decile {b}: orig={acc_orig_by[b]:.4f} | jitter={acc_jit_by[b]:.4f} | drop={drop_by[b]:+.4f}\")\n",
        "plt.figure()\n",
        "plt.plot(range(10), acc_orig_by, marker=\"o\", label=\"Original\")\n",
        "plt.plot(range(10), acc_jit_by, marker=\"o\", label=\"ColorJitter\")\n",
        "plt.title(\"Accuracy by TSI decile under style-only perturbation\")\n",
        "plt.xlabel(\"TSI decile (0 low → 9 high)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.grid(True); plt.legend(); plt.show()\n",
        "plt.figure()\n",
        "plt.plot(range(10), drop_by, marker=\"o\")\n",
        "plt.title(\"Accuracy drop under ColorJitter vs TSI decile\")\n",
        "plt.xlabel(\"TSI decile (0 low → 9 high)\")\n",
        "plt.ylabel(\"Accuracy drop (orig - jitter)\")\n",
        "plt.grid(True); plt.show()\n",
        "fragile = ((base_corr == 1) & (j_corr == 0)).astype(np.int32)  # correct -> wrong under jitter\n",
        "if fragile.mean() > 0:\n",
        "    au_frag_tsi  = roc_auc_score(fragile, tsi)\n",
        "    au_frag_conf = roc_auc_score(fragile, -conf)\n",
        "    print(f\"\\nRobustness-failure AUROC (predict correct→wrong under jitter):\")\n",
        "    print(f\"  TSI:        {au_frag_tsi:.4f}\")\n",
        "    print(f\"  Confidence: {au_frag_conf:.4f}\")\n",
        "    fprf1, tprf1, _ = roc_curve(fragile, tsi)\n",
        "    fprf2, tprf2, _ = roc_curve(fragile, -conf)\n",
        "    plt.figure()\n",
        "    plt.plot(fprf1, tprf1, label=f\"TSI (AUROC={au_frag_tsi:.3f})\")\n",
        "    plt.plot(fprf2, tprf2, label=f\"Confidence (AUROC={au_frag_conf:.3f})\")\n",
        "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
        "    plt.title(\"Robustness-failure ROC (correct→wrong under ColorJitter)\")\n",
        "    plt.xlabel(\"False positive rate\"); plt.ylabel(\"True positive rate\")\n",
        "    plt.grid(True); plt.legend(); plt.show()\n",
        "else:\n",
        "    print(\"\\nRobustness-failure label has zero positives in this subset (increase N or jitter strength).\")\n",
        "# ----------------------------\n",
        "# Visual check: show some examples (low/mid/high TSI)\n",
        "# ----------------------------\n",
        "def show_examples(idxs, title):\n",
        "    rows = len(idxs)\n",
        "    plt.figure(figsize=(14, 3.0*rows))\n",
        "    for r, idx in enumerate(idxs):\n",
        "        x, y = raw_imgs[idx], raw_y[idx]\n",
        "        rng = np.random.default_rng(1234 + idx)  # deterministic so screenshots are stable\n",
        "        xs = shape_view_uint8(x, rng)\n",
        "        xt = texture_view_uint8(x, rng, CFG[\"tile_sizes\"])\n",
        "        xj = j_imgs[idx]\n",
        "        p0, pj = base_prob[idx], j_prob[idx]\n",
        "        info = f\"idx={idx} y={y} TSI={tsi[idx]:+.2f}\\norig p={p0:.2f} corr={base_corr[idx]} | jitter p={pj:.2f} corr={j_corr[idx]}\"\n",
        "        for c, (im, name) in enumerate([(x,\"x\"), (xs,\"S(x)\"), (xt,\"T(x)\"), (xj,\"jitter(x)\")]):\n",
        "            ax = plt.subplot(rows, 4, r*4 + c + 1)\n",
        "            ax.imshow(im); ax.axis(\"off\")\n",
        "            ax.set_title(name if c else (name + \"\\n\" + info), fontsize=9)\n",
        "    plt.suptitle(title, y=1.02); plt.tight_layout(); plt.show()\n",
        "ord_tsi = np.argsort(tsi)\n",
        "k = min(CFG[\"viz_k\"], N//5)\n",
        "low  = ord_tsi[:k]\n",
        "high = ord_tsi[-k:][::-1]\n",
        "mid  = ord_tsi[N//2 - k//2 : N//2 - k//2 + k]\n",
        "show_examples(low,  \"Lowest TSI (more texture-favored by this index)\")\n",
        "show_examples(mid,  \"Middle TSI (often where base accuracy can dip)\")\n",
        "show_examples(high, \"Highest TSI (often where style fragility can be high)\")\n",
        "print(\"\\nDone.\")\n"
      ]
    }
  ]
}